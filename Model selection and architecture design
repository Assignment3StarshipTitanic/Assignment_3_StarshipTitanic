Objective:

The goal of this step is to identify the most suitable machine learning models for predicting wheather a passenger was transported on the spaceship titanic.
The model had to balance accuracy, interpretability, training speed, and generalization.

Models under consideration:

Logistic regression - Simple, interpretable, and fast to train, may not be suitable or underperform for non-linear data.
XGBoost             - Complex, requires more tuning, can handle missing values, and shows complex interactions between data.
Random Forest       - Can handle non-linearity, large model size, less interpretable.
CatBoost            - Can handle Categorical features, longer training time.
SVM                 - Explored briefly, not ideal due to scale and interpretability issues.

Final model choices:

We selected two models based on experimentation and evaluation:

These two models are later used for an ensemble prediction to improve performance and reduce the bias-variance trade-off.

Design:

Data Characteristic- 
The data included:
- Mixed categories (Categorical and numeric)
- It has missing values
- Imbalanced use of ship services
Tree-based models handle these characteristics well.

We selected a Voting Ensemble combining:
- CatBoostClassifier: Strong with categorical data and requires minimal encoding.
- LightGBMClassifier: Extremely efficient, robust, and interpretable with SHAP.
This voting approach leverages the strengths of both models and offers improved generalization, mitigating overfitting by reducing model variance.

Feature engineering by model selection :

- Features like Totalspend, GroupSize, and Deck introduced non-linear patterns, which guided us towards tree-based models(CatBoost, LightGBM)
- Transformed Cabin into structured components (Deck, Side, CabinNum) and filled missing values with defaults.
- Binned continuous Age into categorical AgeGroup for interaction modeling.
- Applied log transformation (LogSpend) to reduce skewness in expenditure features.

Preprocessing Pipeline:

- Imputation: Used median or 0 for numerical columns; default strings/booleans for categorical.
- Label Encoding for tree model compatibility (CatBoost and LGBM supports this natively).
- Standardization applied post-feature generation for model compatibility.

Model Training & Evaluation:

- Dataset split using stratified sampling to avoid class imbalance.
- Performance evaluated using accuracy, confusion matrix, and classification report.
- Visualized model comparison and SHAP-based explainability for trust and transparency.

we did not use any deep learning models and used SHAP due to the considerable resource limitation on google colab





