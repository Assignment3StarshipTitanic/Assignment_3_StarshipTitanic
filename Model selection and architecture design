Objective:

The goal of this step is to identify the most suitable machine learning models for predicting wheather a passenger was transported on the spaceship titanic.
The model had to balance accuracy, interpretability, training speed, and generalization.

Models under consideration:

Logistic regression - Simple, interpretable, and fast to train, may not be suitable or underperform for non-linear data.
XGBoost             - Complex, requires more tuning, can handle missing values, and shows complex interactions between data.
Random Forest       - Can handle non-linearity, large model size, less interpretable.
CatBoost            - Can handle Categorical features, longer training time.
SVM                 - Explored briefly, not ideal due to scale and interpretability issues.

Final model choices:

We selected two models based on experimentation and evaluation:
Logistic regression (Logistic regression)
XGBoost Classifier (Primary model with grid search and tuning)
These two models are later used for an ensemble prediction to improve performance and reduce the bias-variance trade-off.

Design:

Feature engineering by model selection -
Features like Totalspend, GroupSize, and Deck introduced non-linear patterns, which guided us towards tree-based models(XGBoost, Random forest)

Data Characteristic- 
The data included:
- Mixed categories (Categorical and numeric)
- It has missing values
- Imbalanced use of ship services
Tree-based models handle these characteristics well.

Explanability:
- Logistic regression was retained because of its simplicity and explainability - useful for comparision and SHAP value interpretation.

we did not use any deep learning models and used SHAP due to the considerable resource limitation on google colab





